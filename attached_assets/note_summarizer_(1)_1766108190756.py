# -*- coding: utf-8 -*-
"""Note Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kCaJ76q6kS_BOumDUVbqqKnmJxrk7u4P
"""

from google.colab import drive
drive.mount('/content/drive', force_remount= True)

# Commented out IPython magic to ensure Python compatibility.
# %pip install numpy torch transformers[torch] datasets PyPDF2 python-docx -q

"""#Import Libraries and Load AI Model"""

import io
import os
from transformers import pipeline
from PyPDF2 import PdfReader
from docx import Document
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
nltk.download('punkt_tab')

try:
    print("Loading the summarization model...")
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    print("Model loaded successfully!")
except Exception as e:
    print(f"Error loading model: {e}")

import torch
torch.cuda.is_available()
import re

"""# Text Extraction Helper Functions"""

def extract_text_from_pdf(filepath, chunk_length, overlap_length) -> str:
    """Extracts text from a PDF."""
    try:
        reader = PdfReader(filepath)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
            print(page.extract_text())
        text = re.sub(r'\s+', ' ', text).strip()
        text_words = len(text.split(" "))
        print(text_words)
        # print(text)
        chunks = chunk_by_sentences(text, chunk_length, overlap_length)

        print(len(chunks))
        return chunks

    except Exception as e:
        print(f"Error Text Not Extracting Properly from PdfReader: {e}")

def extract_text_from_docx(filepath, chunk_length, overlap_length) -> str:
    """Extracts text from a DOCX."""
    #Document.paragraphs -> paragraphs in our doc (list of paragraphs)
    #Document.paragraph[0].text -> text in each paragraph
    try:
        document = Document(filepath)
        text = ""
        for paragraph in document.paragraphs:
            text += paragraph.text
        text = re.sub(r'\s+', ' ', text).strip()
        text_words = len(text.split(" "))
        print(text_words)
        # print(text)
        chunks = chunk_by_sentences(text, chunk_length, overlap_length)

        print(len(chunks))
        return chunks

    except Exception as e:
        print(f"Error Text Not Extracting Properly from DocX: {e}")

"""# Chunking Helper Function"""

#@title Pseudocode for Chunking Function

# function chunk_by_sentences_with_overlap(text, max_words=600, overlap=100):
#     sentences = split text into sentences
#     chunks = []
#     current_chunk = []
#     word_count = 0

#     for sentence in sentences:
#         sentence_words = count words in sentence

#         # if adding this sentence would overflow the chunk
#         if word_count + sentence_words > max_words:
#             add current_chunk to chunks

#             # start new chunk with overlap
#             overlap_sentences = last sentences of current_chunk
#                                 until their total word count >= overlap
#             current_chunk = overlap_sentences
#             word_count = total words in overlap_sentences

#         add sentence to current_chunk
#         word_count += sentence_words

#     # add any leftover sentences
#     if current_chunk not empty:
#         add current_chunk to chunks

#     return chunks


# function get_overlap_sentences(sentences, overlap):
#     overlap_chunk = empty list
#     word_count = 0

#     # Loop through sentences in reverse order (from most recent backwards)
#     for sentence in sentences reversed:
#         sentence_words = count words in sentence

#         # If adding this sentence would exceed overlap limit, stop
#         if word_count + sentence_words > overlap:
#             break

#         # Prepend the sentence to the overlap list (so order is preserved)
#         insert sentence at beginning of overlap_chunk

#         # Update word count
#         word_count += sentence_words

#     return overlap_chunk

def get_overlap_sentences(sentences, overlap):
    overlap_chunk = []
    count = 0

    #iterate backwards
    for sentence in reversed(sentences):
        sentence_words = len(sentence.split(" "))

        if count + sentence_words > overlap:
            break

        overlap_chunk.insert(0, sentence)

        count += sentence_words

    return overlap_chunk


def chunk_by_sentences(text, length=600, overlap=50):
    # print(text)
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []
    word_count = 0


    for sentence in sentences:
        sentence_words = len(sentence.split(" "))

        if word_count + sentence_words > length:
            chunks.append(" ".join(current_chunk))

            current_chunk = get_overlap_sentences(current_chunk, overlap)
            print("Word count before")
            word_count = sum(len(s.split()) for s in current_chunk)
            print("Word count WORKS")


        current_chunk.append(sentence)
        word_count += sentence_words

    if len(current_chunk) != 0:
        chunks.append(" ".join(current_chunk))

    return chunks

"""# File Upload"""

filepath_environment = "/content/drive/MyDrive/Curious Cardinals/Piyush Nagesh/Essay_ The Risks of Climate Change.docx"

chunks = extract_text_from_docx(filepath_environment, 500, 50)
#print(chunks)

chunks[4]

type(chunks[4])

length = 0

for i in range(len(chunks)):
    length += len(chunks[1][i].split(' '))

print(length)

"""# Generate and Print Summary"""

summary = ""

for i, chunk in enumerate(chunks):
    print(f"--------------Chunk {i}--------------")
    print(chunk)
    summary += " " + summarizer(chunk)[0]["summary_text"]

summary